
# Distributed Multi-Chatroom Messaging System

This is a distributed messaging system built in Python using sockets and threading. It supports multiple clients, chatrooms, a chatbot powered by OpenRouter (GPT-3.5), and includes performance analysis with latency visualization.

---

## ğŸ”§ Features

- ğŸ§  **Multi-client, multi-threaded server** using TCP sockets
- ğŸ’¬ **Multiple chatrooms** with command support:
  - `/join <room>` â€” join or create a chatroom
  - `/leave` â€” leave current room
  - `/list` â€” show all active chatrooms
  - `/quit` â€” disconnect
- ğŸ¤– **Chatbot (AI-enhanced)** that responds intelligently using the OpenRouter API (GPT-3.5)
- ğŸ“Š **Performance testing** with `testsimulation.py` simulating client load
- ğŸ“ˆ **Latency plot** generated with `latency_plot.py` (outputs `latency_plot.png`)
- ğŸ§ª **Latency data file**: `latency_data.txt`
- ğŸ“¦ **SQLite database** stores user accounts and chat messages

---

## ğŸ“ File Structure

```
DistributedChat327/
â”‚
â”œâ”€â”€ server.py # Multi-threaded server
â”œâ”€â”€ client.py # User-facing chat client
â”œâ”€â”€ chatroom.py # Chatroom management logic
â”œâ”€â”€ chatbot.py # Chatbot using OpenRouter API
â”œâ”€â”€ testsimulation.py # Simulates multiple clients and records latency
â”œâ”€â”€ latency_plot.py # Plots latency data into latency_plot.png
â”œâ”€â”€ latency_data.txt # Raw latency results from simulation
â”œâ”€â”€ latency_plot.png # Visualized latency graph
â”œâ”€â”€ chat.db # SQLite database with user/message logs
â”œâ”€â”€ web_frontend/ # WebSocket frontend with Flask
â””â”€â”€ README.md # You're here
```

---

## ğŸš€ How to Run

### Terminal Tab 1: Start Server
```bash
python3 server.py
```

### Terminal Tab 2: Client 1
```bash
python3 client.py
/join testroom
hello
```

### Terminal Tab 3: Client 2
```bash
python3 client.py
/join testroom
how are you?
```

### Terminal Tab 4: Chatbot (AI-Powered)
```bash
python3 chatbot.py
```

- Make sure you insert your OpenRouter API key in `chatbot.py`:
```python
openai.api_key = "sk-or-XXXXXXXXXXXXXXXXXXXXXXXX"
openai.api_base = "https://openrouter.ai/api/v1"
```

### Terminal Tab 5: Load Test + Latency Plot
```bash
python3 testsimulation.py
python3 latency_plot.py
open latency_plot.png
```
## ğŸŒ Web Frontend (Real-Time Chat)

To run the browser-based chat:

1. Make sure `server.py` is running
2. Start the frontend:
   ```bash
   cd web_frontend
   python3 websocket_app.py --port=5050

## ğŸ“¦ Database (SQLite)
The system uses a lightweight SQLite database (chat.db) to persist:

âœ… Registered user accounts

ğŸ’¬ Message history by chatroom

ğŸ“… Timestamps for each message

ğŸ“‚ Tables
users

id (INT, Primary Key)

username (TEXT, Unique)

messages

id (INT, Primary Key)

sender (TEXT)

room (TEXT)

content (TEXT)

timestamp (TEXT)

ğŸ§ª How to View Your Data
1. Run this in Terminal from your project folder:

bash
Copy
Edit
sqlite3 chat.db
Inside the SQLite prompt:

sql
Copy
Edit
.tables
SELECT * FROM users;
SELECT * FROM messages ORDER BY id DESC LIMIT 10;
.exit

---

## ğŸ“¡ API Notes

This project uses [OpenRouter.ai](https://openrouter.ai) to access GPT-3.5-level models
